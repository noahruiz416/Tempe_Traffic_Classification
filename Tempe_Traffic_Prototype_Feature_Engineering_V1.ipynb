{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tempe_Traffic_Prototype_Feature_Engineering_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FrsVnlTZJjP0",
        "tjdozuCTJtpE"
      ],
      "authorship_tag": "ABX9TyOhwpvVj3bNjzDiboD5ylKi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noahruiz416/Tempe_Traffic_Classification/blob/main/Tempe_Traffic_Prototype_Feature_Engineering_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook will contain the initial feature engineering approaches that will be used. Decisions to transform or change certain varaibles will be explained"
      ],
      "metadata": {
        "id": "MNdh2WexOdio"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "5IotTcbuOSf8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"traffic_data.2.csv\")"
      ],
      "metadata": {
        "id": "BJHtOumgOcbJ"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on Exploratory Analysis, I plan on combining certain features that are very similar. Additionally I may exclude certain variables as well. More in depth reasoning will be included in final report."
      ],
      "metadata": {
        "id": "5-YNgcEERsz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "Wi97HV4HS0Fb"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This first set of functions are to be used as needed, depending on the model being ran"
      ],
      "metadata": {
        "id": "FrsVnlTZJjP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function to binary encode accidents as fatal or nonfatal\n",
        "def encode_fatal_accidents(data):\n",
        "  Severity = []\n",
        "  for row in data['Injuryseverity']:\n",
        "    if row != \"Fatal\":\n",
        "      Severity.append(0) #nonfatal\n",
        "    if row == \"Fatal\":\n",
        "      Severity.append(1) #fatal\n",
        "  data['Fatal_Non_Fatal'] = Severity\n",
        "\n",
        "#this function turns our categorical variables into dummy variables\n",
        "def nominal_encoding(data):\n",
        "  data = [['StreetName','Collisionmanner', 'Lightcondition', 'Weather', 'SurfaceCondition']] \n",
        "  data_dummies = pd.get_dummies(data)\n",
        "  return data_dummies\n",
        "\n",
        "def drop_useless_cols(data):\n",
        "  return_data = data.drop(labels = ['X', 'Y', 'OBJECTID',\n",
        "                      'Incidentid', 'DateTime', 'Year',\n",
        "                       'Latitude', 'Longitude', 'Totalinjuries',\n",
        "                       'Totalfatalities', 'Injuryseverity'], axis = 1)\n",
        "  return return_data"
      ],
      "metadata": {
        "id": "j-dSyytqDmCO"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#helper function that contains the logic for whether or not drugs were involved in an accident\n",
        "def label_drugs(row):\n",
        "  if row['DrugUse_Drv1'] == \"Drugs\" or row['DrugUse_Drv2'] == \"Drugs\":\n",
        "    return 1 #drugs are involved\n",
        "  if row['DrugUse_Drv1'] != \"Drugs\" or row['DrugUse_Drv2'] != \"Drugs\":\n",
        "    return 0 #drugs are not involved\n",
        "\n",
        "#helper function that contains the logic for whether or not alch was involved in an accident\n",
        "def label_alchol(row):\n",
        "  if row['AlcoholUse_Drv1'] == \"Drugs\" or row['AlcoholUse_Drv2'] == \"Drugs\":\n",
        "    return 1 #drugs are involved\n",
        "  if row['AlcoholUse_Drv1'] != \"Drugs\" or row['AlcoholUse_Drv2'] != \"Drugs\":\n",
        "    return 0 #drugs are not involved\n",
        "\n",
        "def apply_drug_alchol_label(input_data):\n",
        "  data1 = input_data['drugs_involved'] = input_data.apply (lambda row: label_drugs(row), axis=1)\n",
        "  data1 = input_data['alcohol_involved'] = input_data.apply (lambda row: label_alchol(row), axis=1)\n"
      ],
      "metadata": {
        "id": "99KvumfLJiHe"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### These functions are a part of basic data cleaning and will be used for all train datasets regardless of the model"
      ],
      "metadata": {
        "id": "tjdozuCTJtpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function drops bad columns in light conditions, violations and collisionmanner\n",
        "def drop_useless_rows(input_data):\n",
        "  data1 = input_data[input_data['Lightcondition'].str.contains(\"51\")==False]\n",
        "  data2 = data1[data1['Lightcondition'].str.contains(\"Unknown 51\")==False]\n",
        "  data3 = data2[data2['Violation1_Drv1'].str.contains(\"108\")==False]\n",
        "  data4 = data3[data3['Violation1_Drv1'].str.contains(\"109\")==False]\n",
        "  data5 = data4[data4['Violation1_Drv1'].str.contains(\"49\")==False]\n",
        "  data6 = data5[data5['Collisionmanner'].str.contains(\"10\")==False]\n",
        "  return data6"
      ],
      "metadata": {
        "id": "ld-CjpzrCbdJ"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this function removes outlier driving ages in the data_set and then calculates the median age between the two drivers and creates a new column\n",
        "def age_manipulation(input_data):\n",
        "  data = input_data.drop(input_data.index[input_data['Age_Drv1'] >= 100], inplace=False)\n",
        "  data1 = data.drop(data.index[data['Age_Drv2'] >= 100], inplace=False)\n",
        "  data1['median_age'] = (data1['Age_Drv1'] + data1['Age_Drv2']) / 2\n",
        "  return data1\n"
      ],
      "metadata": {
        "id": "P_z4br8KDFJ9"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to call all the cleaning methods at once\n",
        "def clean_dataset(input_data):\n",
        "  input_data_rows_dropped = drop_useless_rows(input_data)\n",
        "  age_manipulated_data = age_manipulation(input_data_rows_dropped)\n",
        "  return age_manipulated_data"
      ],
      "metadata": {
        "id": "BfjbdRK6EUgZ"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Once we have our data loaded and cleaned we can begin the modeling process"
      ],
      "metadata": {
        "id": "nvGeUytxJ15_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the encode function simply creates our binary labels for fatal or non fatal accidents\n",
        "encode_fatal_accidents(df)\n",
        "clean_dat = clean_dataset(df)\n",
        "clean_dat2 = drop_useless_cols(clean_dat)\n",
        "clean_dat2.dropna(inplace=True)\n",
        "\n",
        "#we then load the data into to different dataframes one with our input vector and the other with train/test labels\n",
        "X = clean_dat2.iloc[:,:-1]\n",
        "y = clean_dat2['Fatal_Non_Fatal']"
      ],
      "metadata": {
        "id": "xqqP8ihTAMdO"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using logistic regression as a baseline model with many input variables\n",
        "from sklearn.linear_model import LogisticRegressionCV"
      ],
      "metadata": {
        "id": "cxmiUjWUKq0e"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in our first baseline approach we will simply OHE all variables and use L1 penalty to shrink coeffcients to 0\n",
        "baseline_x_ohe = pd.get_dummies(X)\n",
        "logit = LogisticRegressionCV(cv=5, random_state=0, penalty = \"l1\", solver='liblinear').fit(baseline_x_ohe, y)"
      ],
      "metadata": {
        "id": "gxptknYSOTt6"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logit.predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vw1SjWYR3_E",
        "outputId": "9294421f-d478-4602-bd57-20849913939f"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xgtXDTQoR61U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}